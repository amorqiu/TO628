---
title: "Telecom Customer Churn Prediction"
author: TO 628 Group 8
date: 4/14/2020
output: html_document
---

## Introduction

The objective of this project is to predict churn or no churn in order to retain the customers in the telco company. The reason we choose this topic is that customer churn is one of the most important metrics for a business to evaluate. Customer churn impedes growth, so companies should have a sense of which kind of group are going to stop to use its product / service.

### About the data (from data description on Kaggle)

https://www.kaggle.com/blastchar/telco-customer-churn

The data set includes information about:

- Customers who left within the last month – the column is called Churn
- Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies
- Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges
- Demographic info about customers – gender, age range, and if they have partners and dependents

```{r}
library(dplyr)
library(corrplot)
library(randomForest)
library(caret)
library(kernlab)
library(neuralnet)
library(gmodels)
library(C50)
```



### Data Exploration

```{r}
data <- read.csv("Churn.csv")
```

```{r}
str(data)
```


```{r}
summary(data)
```


**Factor**
```{r}
# Factor seniorcitizen
data$SeniorCitizen <- as.factor(data$SeniorCitizen)
```


**Missing Values**
There are only 11 missing values in TotalCharges column, the number of missing values is relatively small, so we decide to remove those 11 rows.
```{r}
# remove the rows with missing value
data <- na.omit(data)
```


```{r}
data %>%
  dplyr::select (TotalCharges, MonthlyCharges, tenure) %>%
  cor() %>%
  corrplot.mixed(upper = "circle", tl.col = "black", number.cex = 0.7)
```
Correlation between tunure and TotalCharge, TotalCharges and MonthlyCharges



```{r}
#set bins of tenure 
data$tenure[data$tenure >=0 & data$tenure <= 12] <- '0-1 year'
data$tenure[data$tenure >=12 & data$tenure <= 24] <- '1-2 year'
data$tenure[data$tenure >=24 & data$tenure <= 36] <- '2-3 year'
data$tenure[data$tenure >=36 & data$tenure <= 48] <- '3-4 year'
data$tenure[data$tenure >=48 & data$tenure <= 60] <- '4-5 year'
data$tenure[data$tenure >=60 & data$tenure <= 72] <- '5-6 year'

data$tenure <- as.factor(data$tenure)
```

```{r}
#Remove customerID
data$customerID <- NULL
```

```{r}
# replace the categorical feature "No phone service" and "No Internet Service Service" to "No"
data <- data.frame(lapply(data, function(x) {
                  gsub("No internet service", "No", x)}))

data <- data.frame(lapply(data, function(x) {
                  gsub("No phone service", "No", x)}))

num_columns <- c("MonthlyCharges", "TotalCharges")
data[num_columns] <- sapply(data[num_columns], as.numeric)
```


```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
data[num_columns] <- sapply(data[num_columns], normalize)
```



```{r}
set.seed(111)

# Split data, 80% distribution of churn for training
train.index <- createDataPartition(
    y = data$Churn, p = 0.8, list = FALSE
)
train <- data[train.index,]
test <- data[-train.index,]

```



```{r}
#Simple logistic regression
logit.model <- glm(Churn ~ ., data = train, family = "binomial")
summary(logit.model)
```


```{r}
pred <- predict(logit.model, type = "response", newdata = test)
summary(pred)
#test$prob <- pred

# Using probability cutoff of 50%.
logit.pred_churn <- factor(ifelse(pred >= 0.5, "Yes", "No"))
actual_churn <- factor(ifelse(test$Churn == "Yes","Yes","No"))



CrossTable(test$Churn, logit.pred_churn,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```
```
Through the simple logistic regression, where all variables are included, we can see tenure, contract and internet service are having significant impact on the prediction result. Especially for tenure, the increase in year is correlated with declined probability of customer churn. The model reaches an accuracy of 80.07%, with an AIC of 4750.5.
```

```{r}
#Improved logistic regression
logit.model2 <- glm(Churn ~ tenure + InternetService + SeniorCitizen + PhoneService + MultipleLines + OnlineSecurity + TechSupport + StreamingTV + Contract + PaperlessBilling + PaymentMethod + MonthlyCharges, data = train, family = binomial) 

summary(logit.model2)
```

```{r}
pred2 <- predict(logit.model2, type = "response", newdata = test)
summary(pred2)
#test$prob <- pred

# Using probability cutoff of 50%.
logit.pred_churn2 <- factor(ifelse(pred2 >= 0.5, "Yes", "No"))
actual_churn2 <- factor(ifelse(test$Churn == "Yes","Yes","No"))

CrossTable(test$Churn, logit.pred_churn2,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```
```
For the improved logistic regression model, only certain variables are included. The variables are selected if they have low p-value (<0.05). Also, the variables included here have higher coefficient. However, there is only a slight improvement in the performance: the accuracy for this model is 80.35%, and the AIC is 4748. 
```

```{r}
accuracy <- function(predicted, trueval, model, hideoutput = F) {
  stopifnot(length(predicted) == length(trueval))
  result <- sum(predicted == trueval) / length(predicted)
  if (!hideoutput) {cat("Model:", model, "had", result, "accuracy\n")}
  return(result)
}
```



```{r}
decisionTree <- C5.0(train[-20], train$Churn)
decisionTree_pred <- predict(decisionTree, test)
CrossTable(test$Churn, decisionTree_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c('actual default', 'predicted default'))
```
```{r}
acc_dt = accuracy(decisionTree_pred, test$Churn, "Random Forest Classification", TRUE)
acc_dt
```

```
Through the Decision Tree model, where all variables are included. By using C5.0 formula, the model has an accuracy of 77.94% which is a little bit lower than other models.
```


```{r}
#RandomForest Model
rf.model <- randomForest(Churn ~ ., data = train)
rfpred <- predict(rf.model, newdata = test)
#summary(rfpred)
CrossTable(test$Churn, rfpred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c('actual default', 'predicted default'))
```


```{r}
acc_rf = accuracy(rfpred, test$Churn, TRUE)
acc_rf
```

```
Tring to improve Decision Tree model by using a Random Forest model, where all variables are included. The model has an accuracy of 78.01% which is a little bit better than the Decision Tree model.
```



```{r}
boost100 <- C5.0(train[-20], train$Churn, trials = 100)
boost_pred100 <- predict(boost100, test)
CrossTable(test$Churn, boost_pred100, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,dnn = c('actual default', 'predicted default'))
```

```{r}
acc_rf_boost = accuracy(boost_pred100, test$Churn, "Random Forest Classification", TRUE)
acc_rf_boost
```
```
Tring to improve Decision Tree model by adding boost to the decision tree model, also include all variables . The model has an accuracy of 78.80% which is a little bit better than the Decision Tree model and Random Forest model. However, all three models still lower than other models, which means this dataset may not fit with using Tree prediction models to predict.
```


```
SVM's
```

```{r}
# SVM model
churn_classifier <- ksvm(Churn ~ ., data = train, kernel = "besseldot")
# predictions on testing dataset
churn_predictions <- predict(churn_classifier, test)
CrossTable(test$Churn, churn_predictions,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```
```{r}
# look only at agreement vs. non-agreement
# construct a vector of TRUE/FALSE indicating correct/incorrect predictions
agreement <- churn_predictions == test$Churn
table(agreement)
prop.table(table(agreement))
```


```{r}
# different kernels are different mathmetical functions, the second SVM model
churn_classifier_van <- ksvm(Churn ~ ., data = train, kernel = "vanilladot")
churn_predictions_van <- predict(churn_classifier_van, test)
agreement_van <- churn_predictions_van == test$Churn
table(agreement_van)
prop.table(table(agreement_van))
```
```{r}
CrossTable(test$Churn, churn_predictions_van,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```
```
We tried 8 kernels and assessed the model by prediction accuracy and CrossTable result. We focus more on the number that model predict no but actual yes.Becuase we would like to use this model to target people who are going to stop the service and take actions to retain customers,it’s more serious if we predict they won’t stop the service but actually they will.The SVM with besseldot kernel has the best result.
```

```
KNN
```

```{r}
library(class)

train_label<-train$Churn
test_label<-test$Churn


knn_train_data <- as.data.frame(model.matrix(~ . -1,data=train))
knn_train_data<-knn_train_data[-20]

knn_test_data <- as.data.frame(model.matrix(~ . -1,data=test))
knn_test_data<-knn_test_data[-20]

```

```{r}

test_pred55 <- knn(train = knn_train_data, test = knn_test_data, cl = train_label, k = 55)

CrossTable(test_label, test_pred55)
p1=(1023 + 335) / 1405

```

```{r}
test_pred_3 <- knn(train = knn_train_data, test = knn_test_data, cl = train_label,k = 3)
CrossTable(test_label, test_pred_3)
p2=(338 + 1012) / 1405
```

```{r}
test_pred_21 <- knn(train = knn_train_data, test = knn_test_data, cl = train_label, k = 21)
CrossTable(test_label, test_pred_21)
p3=(1025 + 345) / 1405
```

```{r}
test_pred_59 <- knn(train = knn_train_data, test = knn_test_data, cl = train_label, k = 59)
CrossTable(test_label, test_pred_59)
p4=(1022+334)/1405
```
``````WE tried different k values to make our predictions. When k=55, the accuracy is 96.65%, when k=3, the accuracy is 96.08%, when k=21, the accuracy is 97.5%, and when k=59, the frequently used k value sqrt(sample size *2) the accuracy is 96.5%. When k is smaller. data points are easily be effected by nearby data points. However, if k is large there will be lower variance and higher bias. So we need to make some trade off on bias and accuracy. So we will choose k=21 as our final model where k is relatively in between and the accuracy is good. ```


```{r}
accuracy <- function(predicted, trueval, model, hideoutput = F) {
  stopifnot(length(predicted) == length(trueval))
  result <- sum(predicted == trueval) / length(predicted)
  if (!hideoutput) {cat("Model:", model, "had", result, "accuracy\n")}
  return(result)

}
```



```{r}
a1 = accuracy(logit.pred_churn, test$Churn, "Log Prediction Plain", TRUE)
a2 = accuracy(logit.pred_churn2, test$Churn, "Log Prediction advanced", TRUE)
a3 = accuracy(decisionTree_pred, test$Churn, "Decision Tree", TRUE)
a4 = accuracy(boost_pred100, test$Churn, "Decision Tree with boost100", TRUE)
a5 = accuracy(rfpred, test$Churn, "Random Forest", TRUE)
a6 = accuracy(churn_predictions, test$Churn, "SVM(besseldot) ", TRUE)
a7 = accuracy(churn_predictions_van, test$Churn, "SVM(rdf)", TRUE)
a8 = accuracy(test_pred_21, test$Churn, "KNN(21)", TRUE)
a9 = accuracy(test_pred_59, test$Churn, "KNN(59)", TRUE)

```

```{r}
acc_predictions = c(a1,a2,a3,a4,a5,a6,a7,a8,a9)
names = c("Log Prediction Plain","Log Prediction advanced","Decision Tree","Decision Tree with boost100","Random Forest","SVM(besseldot)","SVM(rdf)","KNN(21)","KNN(59)")

acc_mat <- data.frame(ModelName = names, accuracy = acc_predictions) %>% print
```




```{r}
dotchart(acc_predictions, labels = names, main = "Accuracy of the models", xlab = "Accuracy")
```

The graph above shows the accuracy of the models, We can see that KNN with the k value of 21 has the best predictive accuracy of any of the models, it is interesting that other models are around the 80 %. Random has the lowest acuracy.





We also decide to create a stacked model in order to create a better model. We use ctree to build model and run ctree on top of the model to allow our models to vote.
```{r}
#Use all models to predict train data
log_prediction_train <- predict(logit.model, type = "response", newdata = train)
log_prediction_train <- factor(ifelse(log_prediction_train >= 0.5, "Yes", "No"))


log_prediction_train_advanced <- predict(logit.model2, type = "response", newdata = train)
log_prediction_train_advanced <- factor(ifelse(log_prediction_train_advanced >= 0.5, "Yes", "No"))


decision_prediction_train <- predict(decisionTree, train)
rf_prediction_train <- predict(rf.model, newdata = train)
boost100_prediction_train <- predict(boost100, train)

svm_prediction_train <- predict(churn_classifier, train)
svm_van_prediction_train <- predict(churn_classifier_van, train)


knn59_prediction_train <- knn(train = knn_train_data, test = knn_train_data, cl = train_label, k = 59)
knn21_prediction_train <- knn(train = knn_train_data, test = knn_train_data, cl = train_label, k = 21)
```



```{r}
ConvertToYesNo <- function(myprediction) {
  result <- myprediction %>% as.factor()
  levels(result) <- c("No", "Yes")
  result
}
```


```{r}
logit.pred_churn <- ConvertToYesNo(logit.pred_churn)
logit.pred_churn2 <- ConvertToYesNo(logit.pred_churn2)
decisionTree_pred <- ConvertToYesNo(decisionTree_pred)
boost_pred100 <- ConvertToYesNo(boost_pred100)
rfpred <- ConvertToYesNo(rfpred)
churn_predictions <- ConvertToYesNo(churn_predictions)
churn_predictions_van <- ConvertToYesNo(churn_predictions_van)
test_pred_21 <- ConvertToYesNo(test_pred_21)
test_pred_59 <- ConvertToYesNo(test_pred_59)
```


```{r}
stacked_data = data.frame(logit.pred_churn, logit.pred_churn2, decisionTree_pred, boost_pred100, rfpred, churn_predictions, churn_predictions_van, test_pred_21, test_pred_59,test$Churn) %>% as.tbl()
```


```{r}
model_combined_results <- data.frame(log_prediction_train, log_prediction_train_advanced, decision_prediction_train, rf_prediction_train, boost100_prediction_train, svm_prediction_train, svm_van_prediction_train, knn59_prediction_train, knn21_prediction_train,train$Churn) %>% as.tbl()
```

```{r}
names(model_combined_results) <- names(stacked_data)
```


```{r}
library(party)
#Build ctree to 
stacked_model <- ctree(test.Churn ~ . , data = model_combined_results)
plot(stacked_model)
```


```{r}
stacked_data <- data.frame(stacked_data)
stacked_model_prediction <- predict(stacked_model, stacked_data)
confusionMatrix(as.factor(stacked_model_prediction), as.factor(stacked_data$test.Churn))
```
The model plot shows that stacked ctree model uses decision tree with boosting of 100 trials as the base predictor and adjust some predictions bases on other models. We should expect the accuracy will improve from 80%. The result shows that the accuracy is 95%. But it’s still lower than KNN model.